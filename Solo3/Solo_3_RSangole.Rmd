---
title: "450 - Marketing Analytics - Solo 3"
author: 'R Sangole'
fontsize: 10pt
output:
  pdf_document: 
    df_print: kable
    highlight: tango
    toc: no
  html_notebook: default
urlcolor: blue
---
```{r include=FALSE}
library(doParallel) 
library(tidyverse)
library(janitor)
library(lattice)
library(Amelia)
library(rpart)
library(mice)
library(parallel)
library(caret)
library(pROC)
library(missForest)
library(FSelector)
library(furrr)
library(future)
# -- functions ----
ROCplot <- function(x,t=0.5) {
  plot(
    x,
    print.thres = t,
    type = "S",
    print.thres.pattern = "%.3f (Spec = %.2f, Sens = %.2f)",
    print.thres.cex = 1,
    legacy.axes = TRUE
  )
}
calcAUC <- function(predcol, outcol){
  perf <- ROCR::performance(ROCR::prediction(predcol, outcol==pos),'auc')
  as.numeric(perf@y.values)
}
mkPredC <- function(outCol,varCol,appCol){
  ppos <- sum(outCol==pos)/length(outCol)
  naTab <- table(as.factor(outCol[is.na(varCol)]))
  pposWNA <- (naTab/sum(naTab))[pos]
  vTab <- table(as.factor(outCol),varCol)
  pposWV <- (vTab[pos,]+1e-3*ppos)/(colSums(vTab)+1e-3)
  pred <- pposWV[appCol]
  pred[is.na(appCol)] <- pposWNA
  pred[is.na(pred)] <- ppos
  tibble(X = pred)
}
mkPredN <- function(outcol,varcol,appcol){
  cuts <- unique(as.numeric(quantile(varcol,probs = seq(0,1,0.1),na.rm=T)))
  varc <- cut(varcol,cuts)
  appc <- cut(appcol,cuts)
  mkPredC(outcol,varc,appc)
}
data_read <- function() {
  read_csv('input_data/complete_cust_data.csv', col_names = T)
}
prep_nzv <- function(df, remove_nzv = T) {
  nzvResults <- caret::nearZeroVar(df, foreach = T, saveMetrics = T)
  zv_cols_to_remove <- rownames(nzvResults[nzvResults$zeroVar, ])
  df[zv_cols_to_remove] <- NULL
  if (remove_nzv) {
    nzv_cols_to_remove <- rownames(nzvResults[nzvResults$nzv, ])
    message('Removing columns:', glue::glue_collapse(nzv_cols_to_remove,', '))
    df[nzv_cols_to_remove] <- NULL
  }
  df
}
prep_adultage <- function(df) {
  # Rule: If ADxAGE == U, then ADx_ESTAGE == age, else it's 00
  # Both are NA at the same time
  age_cols <-
    colnames(df) %>% stringr::str_extract(pattern = 'AD[0-9A-Z_]*AGE') %>% na.omit()
  df$AD2AGE = ifelse(df$AD2AGE == 'U', df$AD2_ESTAGE, df$AD2AGE)
  df$AD3AGE = ifelse(df$AD3AGE == 'U', df$AD3_ESTAGE, df$AD3AGE)
  df$AD4AGE = ifelse(df$AD4AGE == 'U', df$AD4_ESTAGE, df$AD4AGE)
  df$AD5AGE = ifelse(df$AD5AGE == 'U', df$AD5_ESTAGE, df$AD5AGE)
  df$AD6AGE = ifelse(df$AD6AGE == 'U', df$AD6_ESTAGE, df$AD6AGE)
  df$AD7AGE = ifelse(df$AD7AGE == 'U', df$AD7_ESTAGE, df$AD7AGE)
  df$AD8AGE = ifelse(df$AD8AGE == 'U', df$AD8_ESTAGE, df$AD8AGE)
  df$AD2_ESTAGE <- NULL
  df$AD3_ESTAGE <- NULL
  df$AD4_ESTAGE <- NULL
  df$AD5_ESTAGE <- NULL
  df$AD6_ESTAGE <- NULL
  df$AD7_ESTAGE <- NULL
  df$AD8_ESTAGE <- NULL
  df
}
prep_adult_g_r <- function(df) {
  df$ADULT2_G <- factor(df$ADULT2_G, levels = c('M', 'F', 'U'))
  df$ADULT3_G <- factor(df$ADULT3_G, levels = c('M', 'F', 'U'))
  df$ADULT4_G <- factor(df$ADULT4_G, levels = c('M', 'F', 'U'))
  df$ADULT5_G <- factor(df$ADULT5_G, levels = c('M', 'F', 'U'))
  df$ADULT6_G <- factor(df$ADULT6_G, levels = c('M', 'F', 'U'))
  df$ADULT7_G <- factor(df$ADULT7_G, levels = c('M', 'F', 'U'))
  
  df$ADULT2_R <-
    factor(df$ADULT2_R, levels = c('H', 'P', 'U', 'W', 'Y'))
  df$ADULT3_R <-
    factor(df$ADULT3_R, levels = c('H', 'P', 'U', 'W', 'Y'))
  df$ADULT4_R <-
    factor(df$ADULT4_R, levels = c('H', 'P', 'U', 'W', 'Y'))
  df$ADULT5_R <-
    factor(df$ADULT5_R, levels = c('H', 'P', 'U', 'W', 'Y'))
  df$ADULT6_R <-
    factor(df$ADULT6_R, levels = c('H', 'P', 'U', 'W', 'Y'))
  df$ADULT7_R <-
    factor(df$ADULT7_R, levels = c('H', 'P', 'U', 'W', 'Y'))
  
  df$ADULT8_G <- NULL
  df$ADULT8_R <- NULL
  
  df
}
prep_removecols <- function(df) {
  df %>%
    dplyr::select(
      -ACCTNO,
      -AMEX_REG,
      -BLOCK,
      -BLOCK_ID,
      -DPBC,
      -FILLER,
      -CRRT,
      -DATEDEED,
      -DPBC,
      -VETERAN,
      -ZHMDECOP,
      # -ZIP4,
      -ZIP9_Supercode,
      -TRACT,
      -MCD_CCD,
      # -LONG,
      # -LAT,
      # -ZIP,
      -PHOMOWNR,
      -LOAN_KND,
      -PHONEMATCH,
      -TRANSTYP
    )
}
prep_to_factor <- function(){

}
prep_tonumeric <- function(df) {
  df$ESTHMVL <- as.numeric(df$ESTHMVL)
  df$ESTLOANTOVALRNG <- as.numeric(df$ESTLOANTOVALRNG)
  df$LOAN_TRM <- as.numeric(df$LOAN_TRM)
  df$EXAGE <- as.numeric(df$EXAGE)
  df$IMPACT <- as.numeric(df$IMPACT)
  df$LOR1 <- as.numeric(df$LOR1)
  df$AD2AGE <- as.numeric(df$AD2AGE)
  df$AD3AGE <- as.numeric(df$AD3AGE)
  df$AD4AGE <- as.numeric(df$AD4AGE)
  df$AD5AGE <- as.numeric(df$AD5AGE)
  df$AD6AGE <- as.numeric(df$AD6AGE)
  df$AD7AGE <- as.numeric(df$AD7AGE)
  df$CTINCIND <- as.numeric(df$CTINCIND)
  df$EXAGE <- as.numeric(df$EXAGE)
  
  df
}
prep_refactor <- function(df) {
  df$ETHNIC_DETAIL <- forcats::fct_lump(df$ETHNIC_DETAIL, prop = 0.01)
  df$ETHNIC_GROUP <- forcats::fct_lump(df$ETHNIC_GROUP, prop = 0.004)
  df$GEOPIXELCODE <- forcats::fct_lump(df$GEOPIXELCODE, prop = 0.004)
  df$M_GLOBAL_Z4 <- forcats::fct_lump(df$M_GLOBAL_Z4, prop = 0.004)
  
  df
}
prep_mutates <- function(df){
  df %>% 
    mutate(
      # How many times has the customer responded in the past, till campaigh 15?
      cusum_resp_till15 = RESPONSE0+RESPONSE1+RESPONSE2+RESPONSE3+RESPONSE4+RESPONSE5+RESPONSE6+RESPONSE7+RESPONSE8+RESPONSE9+RESPONSE10+RESPONSE11+RESPONSE12+RESPONSE13+RESPONSE14+RESPONSE15,
      # How many things were purchased till 15?
      cusum_qty_till15 = QTY+QTY0+QTY1+QTY2+QTY3+QTY4+QTY5+QTY6+QTY7+QTY8+QTY9+QTY10+QTY11+QTY12+QTY13+QTY14+QTY15,
      # What's the total $ purchased till 15?
      cusum_tot_usd_till15 = TOTAMT0+TOTAMT1+TOTAMT2+TOTAMT3+TOTAMT4+TOTAMT5+TOTAMT6+TOTAMT7+TOTAMT8+TOTAMT9+TOTAMT10+TOTAMT11+TOTAMT12+TOTAMT13+TOTAMT14+TOTAMT15,
      cusum_tot_usd_12_to_15 = TOTAMT12+TOTAMT13+TOTAMT14+TOTAMT15,
      old_response_15 = RESPONSE15,
      old_response_14 = RESPONSE14,
      old_response_13 = RESPONSE13,
      old_response_12 = RESPONSE12,
      old_response_11 = RESPONSE11,
      cusum_mailers_till15 = TOTAL_MAIL_15,
      mailers_in_15 = SUM_MAIL_15,
      mailers_in_14 = SUM_MAIL_14,
      PRE2009_SALES = LTD_SALES - YTD_SALES_2009,
      PRE2009_TRANSACTIONS = LTD_TRANSACTIONS-YTD_TRANSACTIONS_2009,
      Y = RESPONSE16
    ) %>% 
    dplyr::select(-LTD_SALES, -YTD_SALES_2009, -LTD_TRANSACTIONS, 
                  -YTD_TRANSACTIONS_2009) %>% 
    dplyr::select(-starts_with('ANY_MAIL')) %>% 
    dplyr::select(-starts_with('SUM_MAIL')) %>% 
    dplyr::select(-starts_with('TOTAL_MAIL')) %>% 
    dplyr::select(-starts_with('QTY')) %>% 
    dplyr::select(-starts_with('TOTAMT')) %>% 
    dplyr::select(-starts_with('RESPONSE')) %>% 
    dplyr::select(-ends_with('16'))
}
prep_cleanups <- function(df) {
  df %>% janitor::clean_names(case = 'all_caps')
}
data_prep_A <- function(df) {
  df %>%
    prep_adultage() %>%
    prep_adult_g_r() %>%
    prep_removecols() %>%
    prep_tonumeric() %>%
    prep_refactor() %>%
    prep_mutates() %>% 
    prep_nzv(remove_nzv = F) %>%
    prep_cleanups()
}
```

```{r eval=FALSE, include=FALSE}
#stuff
raw_data = data_read()
df = data_prep_A(raw_data)
```

# Methodology Discussion

The input data for this modeling exercise is a combination of aggregated customer transaction history data, customer profile information, customer preference information and also demographic data for each zipcodes. The input data is large dataset (30779 rows, each for an individual customer) and 554 variables (numerical and categorical combined).

# Data Preparation

The data required significant pre-preparation before modeling could be done. To summarize the data preparation steps taken:

- Factor variables with `NA` and `U` (Unknown) levels are both re-labeled at `U`
- Numerical variables originally as characters or factors are typecasted appropriately
- Factor variables with either very large number of levels, or with certain levels contributing to very small percentage of the overall data (1% or less) are re-leveled, i.e. levels with very small number of variables are grouped together into `Other`
- Variables with near zero variance are removed. These are identified using the `caret::nearZeroVar()` call. Fully zero variance calls are also removed using this function.
- New variables are created based on the originals. Brief description 
    * cusum_resp_till15 : How many times has the customer responded in the past, till campaign 15?
    * cusum_qty_till15 : How much has the customer purchased in the past, till campaign 15?
    * cusum_tot_usd_till15 : How much has the customer spent in the past, till campaign 15?
    * cusum_tot_usd_12_to_15 : How much has the customer spent in the past, from campaign 12 to campaign 15?
    * old_response_15 : Binary variable (0/1) indicating if the customer reponded for the old campaign
    * old_response_14 : Binary variable (0/1) indicating if the customer reponded for the old campaign
    * old_response_13 : Binary variable (0/1) indicating if the customer reponded for the old campaign
    * old_response_12 : Binary variable (0/1) indicating if the customer reponded for the old campaign
    * old_response_11 : Binary variable (0/1) indicating if the customer reponded for the old campaign
    * cusum_mailers_till15 : Total mailers till campaign 15
    * mailers_in_15 : Mailers mailed in recent past, campaign 15
    * mailers_in_14 : Mailers mailed in recent past, campaign 14
    * PRE2009_SALES : Life-To-Date sales - Year-To-Date-2009 sales
    * PRE2009_TRANSACTIONS : Life-To-Date transactions - Year-To-Date-2009 transactions

## Imputation

There are many variables with missing values in the original dataset. Depending on the nature of the model, NA values may or may not be acceptable. To counter the `NA` values in the dataset, two approaches were taken, at a high level:

1. For numeric variables, the random forest approach in the `mice` package is used, with a repeated imputation of `m=5`. The resulting imputation densities are very close to the original densities. Refer to [figure](#impute_num). A comparison of the blue and red curves show similar curves.
1. For categorical variables, a similar approach was attempted. Various techniques (randomforest, pmm, cart or mean) were attempted, however, I was not succesful in completing this imputation, due to memory & computation constraints of the machine I have. As a result, to simplify the computation, I have two strategies:
  - For levels which contain "U" or "Unknown", the `NA` are recoded to "U"
  - For levels which do not contain "U", `NA` are recoded to "U"

## Data Splits

There are two types of data splits conducted:

1. Train + Test Split for Modeling



1. Train + Calibration + Test for Variable Selection



## Initial Variable Selection

# Model Approach

## Random Forests

## SVM

## Deepboosting

# Modeling Results

Interpreting the average values in [table 2](#overall_beta_table), we can see that:

```{r echo=FALSE, fig.align='center', message=FALSE, warning=FALSE, out.width='70%'}

```

# Final Recommendation to STC

\newpage

# Appendix
 
#### Numerical variables imputation results {#impute_num}
 
```{r echo=FALSE, out.width='80%'}
knitr::include_graphics('images/report_mice_num.png') 
```

\newpage

# Code

```{r eval=FALSE, message=FALSE, warning=FALSE, include=TRUE, paged.print=FALSE}
```