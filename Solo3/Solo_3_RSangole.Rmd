---
title: "450 - Marketing Analytics - Solo 3"
author: 'R Sangole'
fontsize: 10pt
output:
  pdf_document: 
    df_print: kable
    highlight: tango
    toc: no
  html_notebook: default
urlcolor: blue
---

# Methodology Discussion

The input data for this modeling exercise is a combination of aggregated customer transaction history data, customer profile information, customer preference information and also demographic data for each zipcodes. The input data is large dataset (30779 rows, each for an individual customer) and 554 variables (numerical and categorical combined).

# Data Preparation

The data required significant pre-preparation before modeling could be done. To summarize the data preparation steps taken:

- Factor variables with `NA` and `U` (Unknown) levels are both re-labeled at `U`
- Numerical variables originally as characters or factors are typecasted appropriately
- Factor variables with either very large number of levels, or with certain levels contributing to very small percentage of the overall data (1% or less) are re-leveled, i.e. levels with very small number of variables are grouped together into `Other`
- Variables with near zero variance are removed. These are identified using the `caret::nearZeroVar()` call. Fully zero variance calls are also removed using this function.
- New variables are created based on the originals. Brief description 
    * cusum_resp_till15 : How many times has the customer responded in the past, till campaign 15?
    * cusum_qty_till15 : How much has the customer purchased in the past, till campaign 15?
    * cusum_tot_usd_till15 : How much has the customer spent in the past, till campaign 15?
    * cusum_tot_usd_12_to_15 : How much has the customer spent in the past, from campaign 12 to campaign 15?
    * old_response_15 : Binary variable (0/1) indicating if the customer reponded for the old campaign
    * old_response_14 : Binary variable (0/1) indicating if the customer reponded for the old campaign
    * old_response_13 : Binary variable (0/1) indicating if the customer reponded for the old campaign
    * old_response_12 : Binary variable (0/1) indicating if the customer reponded for the old campaign
    * old_response_11 : Binary variable (0/1) indicating if the customer reponded for the old campaign
    * cusum_mailers_till15 : Total mailers till campaign 15
    * mailers_in_15 : Mailers mailed in recent past, campaign 15
    * mailers_in_14 : Mailers mailed in recent past, campaign 14
    * PRE2009_SALES : Life-To-Date sales - Year-To-Date-2009 sales
    * PRE2009_TRANSACTIONS : Life-To-Date transactions - Year-To-Date-2009 transactions
    * lat, and long : For the lattitude and longitude, the left most 4-digit numbers are extracted, and the series are converted to factor variables.

## EDA

Upon exploratory data analysis at a univariate level with respect to the response variable 'RESPONSE16', it was found that the variable `BUYER_STATUS` is a strong indicator of the response of a customer. Status of INACTIVE or LAPSED indicate that the customer will have no response to a campaign. This is a straight rule which can be used for any new data in the test dataset. Going forward, any rows containing the levels other than ACTIVE are removed.

```{r echo=FALSE, message=FALSE, warning=FALSE}
read_csv('bstatus.csv') %>% xtabs(~BUYER_STATUS+RESPONSE16,.)
```

## Imputation

There are many variables with missing values in the original dataset. Depending on the nature of the model, NA values may or may not be acceptable. To counter the `NA` values in the dataset, two approaches were taken, at a high level:

1. For numeric variables, the random forest approach in the `mice` package is used, with a repeated imputation of `m=5`. The resulting imputation densities are very close to the original densities. Refer to [figure](#impute_num). A comparison of the blue and red curves show similar curves.
1. For categorical variables, a similar approach was attempted. Various techniques (randomforest, pmm, cart or mean) were attempted, however, I was not succesful in completing this imputation, due to memory & computation constraints of the machine I have. As a result, to simplify the computation, I have two strategies:
  - For levels which contain "U" or "Unknown", the `NA` are recoded to "U"
  - For levels which do not contain "U", `NA` are recoded to "U"

## Data Splits

There are two types of data splits conducted:

1. *Train + Test Split for Modeling:* The variable `LEARNING_TEST` in the dataset consisted of two levels (LEARNING, TESTING) which were both stratified for the response variable. This was used to generate the split.

1. *Train + Calibration + Test for Variable Selection:* The training split above is further split into a 70-30 stratified split on the reponse variable into a train & calibration dataset. These splits are only used to select and validate the initial set of variables using the techniques described below. Once the variables are defined, the train+test split described above are used for model building & validation.

## Initial Variable Selection

The initial challenge of this problem was to reduce the number of variables from 544 to a more manageable number. Two approaches were attempted:

1. The variables were surveyed manually and using subject matter expertize (names of the variables, descriptions and thus a derived estimation of usefulness), ~120 variables are selected. A set of models is build using these 120 variables. Based on the variable importance plots, these are reduced to ~50 odd variables. The top 5-10 variables from this approach were noted down.
1. Approach two involved utilizing math to reduce the number of variables. The approach followed is described in Zumel & Mount (2014, pg 118 - 122). It involves building a series of single-variable models on the training set (Y ~ Var1, Y~Var2,... Y~VarN). For each model, the AUC on the training set, and the AUC for the calibration set is calculated. The top 20-25 rank ordered calibration-AUC values are selected from each variable types. These are taken into the more detailed analysis. The top 6 variables from each type are shwon below, with their AUC scores. The [appendix](#auc) has graphs showing the comparison of the training and calibration AUC scores; for the categoricals we can see that there are certain variables have a high training score but a low calibration score. In addition to these variables, the manually selected variables from approach 1 are appended.

```{r echo=FALSE, message=FALSE, warning=FALSE}
read_csv('search_nums.csv') %>% head(6) %>% knitr::kable(digits = 3, align = 'c', caption = 'Search across numerical variables')
read_csv('search_cats.csv') %>% head(6) %>% knitr::kable(digits = 3, align = 'c', caption = 'Search across categorical variables')
```

# Model Approach

The package `caret` is used for all the modeling efforts. This package offers a unified interface to over 200 modeling packages. In addition to this, it offers convinient wrappers to perform cross validation, performance metrics extraction and plotting. In total, three models are attempted:

- Randomforest using the `randomForest` package. [Also attempted was the `parRF` which solves random forest in a parallel multi-core approach.]
- Naive Bayes
- Averaged Neural Networks

For each model, different pre-processing steps are testing, which include (but not limited to) centering & scaling variables, YeoJohnson transformation, PCA etc. For each model, tuneable hyperparamters (like `mtry` for randomForests) are selected using 10-fold cross validation on the training dataset, while trying to maximize the ROC AUC value.

# Modeling Results

## Random Forests

The `randomForest` package within `caret` was used to fit multiple models. Three sets of models were build - raw data, scale + centered data, scale + centered + YeoJohnson transformed data. The tuning parameter `mtry` was varied between 

## Naive Bayes

Naive bayes models are fit from the `klaR` package which allow for 3 tunable parameters. `fL` (Laplace Correction): 0, `usekernel` (Distribution Type): {gaussian / nonparametric}, `adjust` (Bandwidth Adjustment): {0.1, 0.2, 0.4, 0.6}. The optimal tuning parameters can be selected using ROC as the criteria:

```{r echo=FALSE, out.width='80%'}
knitr::include_graphics('images/nb_tune.png') 
```


## Neural Networks

The `avnnet` package,  Ripley (1996), fits the same neural network model using different random number seeds for the starting weights of the neurons. All the resulting models are used for prediction. For classification, the model scores are first averaged, then translated to predicted classes. Bagging can also be used to create the models. Other tuning parameters include the number of internal units, and the decay coefficient. 

## Training Results & Performance Evaluation

In total, 50 number of models are build, including the resampled models built for the 10-fold CV effort. The following graph summarizes the performance of them models, assessed by the AUC ROC, sensitivity and specificity. We can see that:

- Between each model's 10-fold resamples on the training data, the numbers are very tightly distributed. This gives us confidence in the stability of the models to transfer to the test dataset.
- Across models, the performance is fairly similar with ... performing the best when measured using the AUC ROC metric. Note: these are calculated using the default probability cutoff values of 0.5.

```{r echo=FALSE, out.width='80%'}
knitr::include_graphics('images/bwplot_resamples.png') 
```

The training set ROC values can be visualized on a density plot for each of the models. Each point is 1 resample within the cross validation. Thus, the following plot is made of 50 models.

```{r echo=FALSE, out.width='80%'}
knitr::include_graphics('images/roc_densityplot.png') 
```

## Test Set Performance Evaluation

So far, the test set has been untouched. Fitting the models to the test dataset, we can plot the performance of the models on the following chart. Key take aways:

```{r echo=FALSE, out.width='80%'}
knitr::include_graphics('images/test_performance.png') 
```

- All the models have carried their performance metrics from train to test. This confirms the belief after cross validation that the models have not overfit to the training data.
- The dashed line at the bottom is the No Information Rate, which is the accuracy achieved if all the observations are simply predicted to be the majority class. Clearly, all the models outperform the NIR.
- Kappa values, which given an indication of how much better our classification (for a 2 class problem) is than chance alone, shows us that the models are of moderate quality. (Desired kappa is > 0.7).
- These results are for the default probability cutoff value of 0.5.

## Evaluation of probability cutoff

```{r echo=FALSE, out.width='80%'}
knitr::include_graphics('images/acc_kappa_testset.png') 
```

# Final Selected model

## Interpretation

## Confusion matrix


# Customer Scoring

Applying selected model to the full dataset...



# Final Recommendation to STC

## Customers to target

## Campaign / Analysis Improvement Ideas

\newpage

# Stretch Goals for this assignment

My personal stretch goals for this assignment were to 

- Understand and apply few methods of imputation using the `mice` package, especially using parallel compute. I was partially able to achive this, but it depended on if I'm using my Mac vs my Windows machine, since the success of `r parallel::makePSOCKcluster()` depends on the underlying operating system's infrastructure.
- Develop the end to end analysis pipeline using a more production ready coding structure. I learnt the basics of using the `drake` package, it's `makePlan` functions which uniquely hashtag each R object and keep tracability as objects pass from function to function.


\newpage

# Appendix

#### Missingness maps {#missmap}

```{r echo=FALSE, out.width='80%'}
knitr::include_graphics('images/cat_missmap.png') 
knitr::include_graphics('images/num_missmap.png') 
```

 
#### Numerical variables imputation results {#impute_num}
 
```{r echo=FALSE, out.width='80%'}
knitr::include_graphics('images/report_mice_num.png') 
```

#### AUC results {#auc}
 
```{r echo=FALSE, out.width='80%'}
knitr::include_graphics('catvar_auc.png')
knitr::include_graphics('numvar_auc.png') 
```

#### Class - Probability Distributions {#classprob}
 
```{r echo=FALSE, out.width='80%'}
knitr::include_graphics('images/rffit.nocs.png')
knitr::include_graphics('images/rffit.nocs.png')
knitr::include_graphics('images/nbFit.cs.png')
knitr::include_graphics('images/avnnet.cs.png')
```

\newpage

# Code

```{r eval=FALSE, message=FALSE, warning=FALSE, include=TRUE, paged.print=FALSE}
```